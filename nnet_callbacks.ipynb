{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr, ttest_rel\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_xgb(preds, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, preds) / gini(y, y)\n",
    "\n",
    "def gini_lgb(preds, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, preds) / gini(y, y), True\n",
    "\n",
    "def categorical_encoding_by_target(data, cat_features, target, inplace=False, \n",
    "                                   regul=5, smoothing=10, random_seed=0):\n",
    "    \n",
    "    encoding_dict = dict()\n",
    "    data['target'] = target\n",
    "    data['myid'] = np.arange(0, data.shape[0], 1)\n",
    "    mean_target = data.target.mean()\n",
    "    N_SPLITS = 5\n",
    "\n",
    "    unique_values_dict = dict()\n",
    "    for col in cat_features:\n",
    "        data[col+'_enc'] = 0\n",
    "        encoding_dict[col] = dict()\n",
    "        unique_values_dict[col] = data[col].unique()\n",
    "\n",
    "    for train_ind, test_ind in KFold(n_splits=N_SPLITS, \n",
    "                                               shuffle=True, \n",
    "                                               random_state=random_seed).split(data, data.target.values):    \n",
    "        for col in cat_features:\n",
    "            tmp_aggregated_df = (data\n",
    "                                 .loc[train_ind]\n",
    "                                 .groupby(col, as_index=False)\n",
    "                                 .agg({'target': 'mean', 'myid':'count'})\n",
    "                                 .set_index(col)\n",
    "                                )\n",
    "            mean_target = data.loc[train_ind].target.mean()\n",
    "            mean_dict    = tmp_aggregated_df.target.to_dict()\n",
    "            counter_dict = tmp_aggregated_df.myid.to_dict()\n",
    "            \n",
    "            for x in unique_values_dict[col]:\n",
    "                if counter_dict.get(x,0) < 10:\n",
    "                    encoding_dict[col][x] = mean_target\n",
    "                    continue        \n",
    "                \n",
    "                \n",
    "#                 encoding_dict[col][x] = encoding_dict[col].get(x,0) + \\\n",
    "#                 (mean_dict.get(x,0)*counter_dict.get(x,0) + mean_target*regul) / (counter_dict.get(x,0)+regul)\n",
    "                \n",
    "                encoding_dict[col][x] = encoding_dict[col].get(x,0) + \\\n",
    "                mean_target * (1 - 1 / (1 + np.exp(-(counter_dict.get(x,0) - regul) / smoothing)))\\\n",
    "                    + mean_dict.get(x,0) * 1 / (1 + np.exp(-(counter_dict.get(x,0) - regul) / smoothing))\n",
    "                    \n",
    "            data.loc[test_ind, col+'_enc'] = data.loc[test_ind, col].apply(lambda x: \n",
    "                   mean_target * (1 - 1 / (1 + np.exp(-(counter_dict.get(x,0) - regul) / smoothing)))\\\n",
    "                    + mean_dict.get(x,0) * 1 / (1 + np.exp(-(counter_dict.get(x,0) - regul) / smoothing)))\n",
    "\n",
    "#             data.loc[test_ind, col+'_enc'] = data.loc[test_ind, col].apply(lambda x: \n",
    "#                    (mean_dict.get(x,0)*counter_dict.get(x,0) + mean_target*regul) / (counter_dict.get(x,0)+regul))\n",
    "            \n",
    "            #test[col+'_enc'] = test[col].apply(lambda x: \n",
    "            #        (mean_dict.get(x,0)*counter_dict.get(x,0) + mean_target*regul) / (counter_dict.get(x,0)+regul))\n",
    "            \n",
    "            #print ('Train OOF:', data.loc[test_ind][['target', col+'_enc']].corr(method='spearman').iloc[0][1].round(4))    \n",
    "            #print ('Test corr:', test[['target', col+'_enc']].corr(method='spearman').iloc[0][1].round(4)) \n",
    "            \n",
    "    for col in cat_features:\n",
    "        for x in encoding_dict[col]:\n",
    "            encoding_dict[col][x] = encoding_dict[col][x]/N_SPLITS\n",
    "        if inplace:\n",
    "            data[col] = data[col+'_enc']\n",
    "            data.drop(col+'_enc', axis=1, inplace=True)\n",
    "    data = data.drop(['target', 'myid'], axis=1)\n",
    "    return data, encoding_dict\n",
    "\n",
    "def categorical_encoding_by_order(data, cat_features, target, inplace=False):\n",
    "    \n",
    "    data['target'] = target\n",
    "    encoding_dict = dict()\n",
    "    postfix = '_enc'*(not inplace) # '' if inplace==True, '_enc' if inplace=True\n",
    "\n",
    "    for col in cat_features:\n",
    "        encoding_dict[col] = dict()\n",
    "\n",
    "        col_sorted = (data[[col, 'target']]\n",
    "                     .groupby(col, as_index=False)\n",
    "                     .agg({'target':np.mean})\n",
    "                     .sort_values('target')\n",
    "                     [col]\n",
    "                    )\n",
    "        \n",
    "        cat_number = 0\n",
    "        for category in col_sorted:\n",
    "            encoding_dict[col][category] = cat_number\n",
    "            cat_number += 1\n",
    "    \n",
    "        data[col+postfix] = data[col].apply(lambda x: encoding_dict[col].get(x, -1))\n",
    "    data = data.drop('target', axis=1)\n",
    "    return data, encoding_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "def get_dist_for_cluster_centers(cluster_num, df):\n",
    "\n",
    "    cluster_col = 'CL_'+str(cluster_num)\n",
    "    dist_col = 'CL_'+str(cluster_num)+'_'\n",
    "    df[cluster_col] = df[cluster_col].fillna(0)\n",
    "\n",
    "    df['CL_dist_'+str(cluster_num)] = [df[dist_col+str(int(c))].iloc[i] \n",
    "                                         for c, i in zip(df[cluster_col], range(len(df)))]\n",
    "    \n",
    "    old_distance_columns = [col for col in df.columns if dist_col in col]\n",
    "    df.drop(old_distance_columns, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train = get_dist_for_cluster_centers(6,  train)\n",
    "train = get_dist_for_cluster_centers(13, train)\n",
    "#train = get_dist_for_cluster_centers(21, train)\n",
    "\n",
    "test = get_dist_for_cluster_centers(6,  test)\n",
    "test = get_dist_for_cluster_centers(13, test)\n",
    "#test = get_dist_for_cluster_centers(21, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    \n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c and add_ranges:\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "    for c in one_hot:\n",
    "        if (len(one_hot[c])>2 and len(one_hot[c]) <= max_categories_for_ohe)\\\n",
    "        and ('_cat' in c or not ohe_cat_only):\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_OH_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "            df = df.drop(c, axis=1)\n",
    "    return df\n",
    "\n",
    "def multi_transform(df):\n",
    "    #print('Init Shape: ', df.shape)\n",
    "    p = mp.Pool(6)\n",
    "    df = p.map(transform_df, np.array_split(df, 6))\n",
    "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    p.close(); p.join()\n",
    "    #print('After Shape: ', df.shape)\n",
    "    return df\n",
    "\n",
    "def recon(reg):\n",
    "    integer = int(np.round((40*reg)**2)) \n",
    "    for a in range(32):\n",
    "        if (integer - a) % 31 == 0:\n",
    "            A = a\n",
    "    M = (integer - A)//31\n",
    "    return A, M\n",
    "\n",
    "def preproc(df, part1=True, part2=True):\n",
    "    \n",
    "    if part1:\n",
    "        df['ps_reg_A'] = df['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "        df['ps_reg_M'] = df['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "        df['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "        df['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "        df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    \n",
    "    if part2:\n",
    "        df['mult']   = df['ps_reg_01'] * df['ps_reg_03'] * df['ps_reg_02']\n",
    "        df['ps_car'] = df['ps_car_13'] * df['ps_reg_03'] * df['ps_car_13']\n",
    "        df['ps_ind'] = df['ps_ind_03'] * df['ps_ind_15']\n",
    "\n",
    "    return df\n",
    "\n",
    "import scipy\n",
    "\n",
    "def get_pval_for_binom_test(p1, p2, n1, n2):\n",
    "    p = (p1*n1 + p2*n2)/(n1+n2)\n",
    "    z_stat = (p1-p2)/np.sqrt( p*(1-p)*(1/n1 + 1/n2) )\n",
    "    pval = min(scipy.stats.norm.cdf(z_stat), 1-scipy.stats.norm.cdf(z_stat))/2\n",
    "    return pval\n",
    "\n",
    "def target_encoding_with_pval(train, target_train, cat, pval):\n",
    "    train['target'] = target_train\n",
    "    train['cnt'] = 0\n",
    "    cat_agg = (train\n",
    "               .groupby(cat, as_index=False)\n",
    "               .agg({'target':'mean'\n",
    "                    ,'cnt':'count'})\n",
    "               .sort_values('target')\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "\n",
    "    group_dict = dict()\n",
    "    \n",
    "    max_pval = 1\n",
    "    while max_pval > pval:\n",
    "\n",
    "        pvals = [get_pval_for_binom_test(\n",
    "                    cat_agg.iloc[i].target, \n",
    "                    cat_agg.iloc[i+1].target, \n",
    "                    cat_agg.iloc[i].cnt, \n",
    "                    cat_agg.iloc[i+1].cnt)\n",
    "                 for i in range(cat_agg.shape[0]-1)\n",
    "                ]\n",
    "        max_pval, max_pval_ind = max(pvals), np.argmax(pvals)\n",
    "\n",
    "        replaced_group  = cat_agg.loc[max_pval_ind]  [cat]\n",
    "        replacing_group = cat_agg.loc[max_pval_ind+1][cat]\n",
    "\n",
    "        group_dict[replaced_group] = replacing_group\n",
    "\n",
    "        cat_agg = cat_agg.set_value(max_pval_ind, cat, cat_agg.loc[max_pval_ind+1][cat])\n",
    "        cat_agg = (cat_agg\n",
    "                   .assign(target = lambda df: df.cnt*df.target)\n",
    "                   .groupby(cat, as_index=False)\n",
    "                   .agg({'target':'sum'\n",
    "                        ,'cnt':'sum'})\n",
    "                   .assign(target = lambda df: df.target/df.cnt)\n",
    "                   .sort_values('target')\n",
    "                   .reset_index(drop=True)\n",
    "                  )\n",
    "        if cat_agg.shape[0] == 1:\n",
    "            break\n",
    "        \n",
    "    all_keys = list(group_dict.keys())\n",
    "\n",
    "    replace_count = 1\n",
    "    while replace_count>0:\n",
    "        replace_count=0\n",
    "        for c in all_keys:\n",
    "            for c_old in group_dict:\n",
    "                if group_dict [c_old] == c:\n",
    "                    group_dict[c_old] = group_dict[c]\n",
    "                    replace_count +=1\n",
    "    train.drop(['target', 'cnt'], axis=1, inplace=True)\n",
    "\n",
    "    return group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(train_series=None, target=None):\n",
    "\n",
    "    temp = pd.concat([train_series, pd.Series(target, name='target')], axis=1)\n",
    "    # Compute target mean\n",
    "    aggregated_values = temp.groupby(by=train_series.name)['target'].agg([\"mean\", \"count\", np.std])\n",
    "    total_std = np.std(target)\n",
    "    aggregated_values[\"std\"].fillna(total_std, inplace=True)\n",
    "\n",
    "    # Compute smoothing\n",
    "    smoothing_component = aggregated_values[\"count\"] * total_std ** 2\n",
    "    smoothing = smoothing_component / (aggregated_values[\"std\"] ** 2 + smoothing_component)\n",
    "\n",
    "    # Apply average function to all target data\n",
    "    mean_total = target.mean()\n",
    "    mean_values = mean_total * (1 - smoothing) + aggregated_values[\"mean\"] * smoothing\n",
    "\n",
    "    mean_values_dict = mean_values.to_dict()\n",
    "\n",
    "    train_columns = train_series.replace(mean_values_dict).fillna(mean_total)\n",
    "\n",
    "    return mean_values_dict\n",
    "\n",
    "def add_iteractions(df):\n",
    "    df['ps_car_13_X_ps_ind_05_cat'] = df['ps_car_13'] + df['ps_ind_05_cat']\n",
    "    df['ps_car_11_cat_X_ps_ind_05_cat'] = df['ps_car_11_cat'] + df['ps_ind_05_cat']\n",
    "    df['ps_ind_05_cat_X_ps_ind_17_bin'] = df['ps_ind_05_cat'] + df['ps_ind_17_bin']\n",
    "\n",
    "    df['ps_car_13_X_ps_ind_17_bin'] = df['ps_car_13'] + df['ps_ind_17_bin']\n",
    "\n",
    "    df['ps_car_11_cat_X_ps_ind_17_bin'] = df['ps_car_11_cat'] + df['ps_ind_17_bin']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOAD_TEST = True\n",
    "\n",
    "train = pd.concat([pd.read_csv('train.csv'),\n",
    "                   #pd.read_csv('train_clusters.csv', usecols=['CL_6', 'CL_13', 'CL_21'])\n",
    "                  ], axis=1)\n",
    "y = train.target\n",
    "features = [c for c in train.columns if c not in ['id', 'target']]\n",
    "print('Train shape:', train.shape)\n",
    "\n",
    "if LOAD_TEST:\n",
    "    test = pd.concat([pd.read_csv('test.csv'),\n",
    "                   #pd.read_csv('test_clusters.csv', usecols=['CL_6', 'CL_13', 'CL_21'])\n",
    "                  ], axis=1)\n",
    "    print('Test shape:', test.shape)\n",
    "else:\n",
    "    train, test, y, ytest = train_test_split(train, \n",
    "                                        train['target'].values, \n",
    "                                        test_size=0.2, \n",
    "                                        random_state=90\n",
    "                                        )\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    target_test = test['target'].values\n",
    "    \n",
    "target_train = train['target'].values    \n",
    "id_train = train['id'].values\n",
    "id_test  = test ['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.iloc[:,2:]\n",
    "test = test.iloc[:,1:]\n",
    "\n",
    "cols_use = [c for c in train.columns if (not c.startswith('ps_calc_'))]\n",
    "\n",
    "train = train[cols_use]\n",
    "test = test[cols_use]\n",
    "\n",
    "col_vals_dict = {c: list(train[c].unique()) for c in train.columns if c.endswith('_cat')}\n",
    "\n",
    "embed_cols = []\n",
    "for c in col_vals_dict:\n",
    "    if len(col_vals_dict[c])>2:\n",
    "        embed_cols.append(c)\n",
    "        print(c + ': %d values' % len(col_vals_dict[c]))\n",
    "embed_cols = sorted(embed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_scale = [c for c in train.columns if c not in embed_cols]\n",
    "scaler = MinMaxScaler()\n",
    "train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])\n",
    "test [cols_to_scale] = scaler.transform(test[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "to_drop_additional = []\n",
    "# for vals in [(False, False, 20, True, False, True, ('pval', 0.02), [], False)\n",
    "#             ,(False, False, 20, True, False, True, ('no_encoding', 0.0), [], False)]:\n",
    "    \n",
    "vals = (False, False, 0, True, False, True, ('no_encoding', 0.0), [], False)\n",
    "(preproc_part1, \n",
    " preproc_part2, \n",
    " max_categories_for_ohe,\n",
    " ohe_cat_only,\n",
    " add_ranges,\n",
    " drop_calc,\n",
    " encode_cats,\n",
    " to_drop_additional,\n",
    " add_interactions) = vals\n",
    "\n",
    "print (vals)\n",
    "\n",
    "train = preproc(train, preproc_part1, preproc_part2)\n",
    "test  = preproc(test,  preproc_part1, preproc_part2)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['id','target']+to_drop_additional]\n",
    "if drop_calc:\n",
    "    features = [c for c in features if not c.startswith('ps_calc_') and c not in ['id','target']]\n",
    "train = train[features+['target']]\n",
    "test  = test [features]\n",
    "\n",
    "bin_features = [col for col in features if train[col].nunique()==2]\n",
    "\n",
    "cat_features = [col for col in features \n",
    "                if col not in bin_features \n",
    "                    and ('bin' in col or 'cat' in col) \n",
    "                    and train[col].nunique()<200\n",
    "                    and sorted(train[col].unique()) == sorted(test[col].unique())\n",
    "                    and '_median_range' not in col\n",
    "                    and '_OH_' not in col]\n",
    "\n",
    "if add_interactions:\n",
    "    scaler = MinMaxScaler()\n",
    "    train[features] = scaler.fit_transform(train[features])\n",
    "    test [features] = scaler.transform(test[features])\n",
    "\n",
    "    train = add_iteractions(train)\n",
    "    test  = add_iteractions(test)\n",
    "    features = list(test.columns)\n",
    "\n",
    "train = train.replace(-1, np.NaN)\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "train = train.fillna(-1)\n",
    "\n",
    "one_hot = {c: list(train[c].unique()) for c in features if c not in ['id','target']}\n",
    "train = multi_transform(train[features])\n",
    "test  = multi_transform(test [features])\n",
    "\n",
    "features = [c for c in train.columns if c not in ['target']]\n",
    "cat_features = [c for c in cat_features if c in features]\n",
    "\n",
    "if encode_cats[0] == 'no_encoding':\n",
    "    cat_features=[]\n",
    "print (cat_features)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_12_bin', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_reg_01', 'ps_reg_02', 'ps_car_01_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_08_cat', 'ps_car_13', 'ps_car_15', 'ps_car_13_x_ps_reg_03', 'negative_one_vals', 'ps_ind_01_median_range', 'ps_ind_01_mean_range', 'ps_ind_02_cat_median_range', 'ps_ind_02_cat_mean_range', 'ps_ind_03_median_range', 'ps_ind_03_mean_range', 'ps_ind_04_cat_median_range', 'ps_ind_04_cat_mean_range', 'ps_ind_05_cat_median_range', 'ps_ind_05_cat_mean_range', 'ps_ind_14_mean_range', 'ps_ind_15_median_range', 'ps_ind_15_mean_range', 'ps_reg_01_median_range', 'ps_reg_01_mean_range', 'ps_reg_02_median_range', 'ps_reg_02_mean_range', 'ps_reg_03_median_range', 'ps_reg_03_mean_range', 'ps_car_01_cat_median_range', 'ps_car_02_cat_mean_range', 'ps_car_03_cat_mean_range', 'ps_car_04_cat_median_range', 'ps_car_04_cat_mean_range', 'ps_car_06_cat_mean_range', 'ps_car_07_cat_mean_range', 'ps_car_08_cat_mean_range', 'ps_car_09_cat_median_range', 'ps_car_12_mean_range', 'ps_car_13_median_range', 'ps_car_13_mean_range', 'ps_car_14_median_range', 'ps_car_14_mean_range', 'ps_car_15_mean_range', 'ps_calc_11_mean_range', 'ps_calc_12_median_range', 'ps_calc_14_median_range', 'ps_reg_M_mean_range', 'ps_car_13_x_ps_reg_03_median_range', 'ps_car_13_x_ps_reg_03_mean_range', 'ps_calc_05_OH_5', 'ps_car_07_cat_OH_1.0', 'ps_car_07_cat_OH_-1.0', 'ps_car_07_cat_OH_0.0', 'ps_car_02_cat_OH_1.0', 'ps_car_02_cat_OH_0.0', 'ps_ind_02_cat_OH_2.0', 'ps_ind_02_cat_OH_1.0', 'ps_car_09_cat_OH_0.0', 'ps_car_09_cat_OH_3.0', 'ps_car_09_cat_OH_1.0', 'ps_car_03_cat_OH_-1.0', 'ps_car_03_cat_OH_1.0', 'ps_ind_14_OH_0', 'ps_car_05_cat_OH_-1.0', 'ps_car_10_cat_OH_1', 'ps_ind_04_cat_OH_1.0', 'ps_ind_04_cat_OH_0.0', 'ps_car_11_OH_3.0', 'ps_car_11_OH_1.0', 'ps_car_11_OH_0.0']\n",
    "features += ['ps_car_11_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc(X_train, X_val, X_test):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_val = []\n",
    "    input_list_test = []\n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(X_train[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(X_train[c].map(val_map).values)\n",
    "        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n",
    "     \n",
    "    #the rest of the columns\n",
    "    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n",
    "    input_list_train.append(X_train[other_cols].values)\n",
    "    input_list_val.append(X_val[other_cols].values)\n",
    "    input_list_test.append(X_test[other_cols].values)\n",
    "    \n",
    "    return input_list_train, input_list_val, input_list_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%env KERAS_BACKEND=theano\n",
    "from keras import callbacks\n",
    "\n",
    "class AUC_SKlearn_callback(callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train, useCv = True):\n",
    "        super(AUC_SKlearn_callback, self).__init__()\n",
    "        self.bestAucCv = 0\n",
    "        self.bestAucTrain = 0\n",
    "        self.cvLosses = []\n",
    "        self.bestCvLoss = 1,\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.useCv = useCv\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_pred = self.model.predict(self.X_train)\n",
    "        aucTrain = roc_auc_score(self.y_train, train_pred)\n",
    "        #print(\"Train AUC: \" + str(aucTrain))\n",
    "\n",
    "        if (self.bestAucTrain < aucTrain):\n",
    "            self.bestAucTrain = aucTrain\n",
    "            #print (\"Best SKlearn AUC training score so far\")\n",
    "            #**TODO: Add your own logging/saving/record keeping code here\n",
    "\n",
    "        if (self.useCv) :\n",
    "#             print (self.validation_data[0].shape)\n",
    "#             print (len(self.validation_data))\n",
    "#             for i in self.validation_data:\n",
    "#                 print (i.shape)\n",
    "            cv_pred = self.model.predict(self.validation_data[:len(embed_cols)+1])\n",
    "            aucCv = roc_auc_score(self.validation_data[len(embed_cols)+1], cv_pred)\n",
    "            #print (\"Valid AUC: \" +  str(aucCv))\n",
    "\n",
    "            if (self.bestAucCv < aucCv) :\n",
    "                # Great! New best *actual* CV AUC found (as opposed to the proxy AUC surface we are descending)\n",
    "                #print(\"Best SKLearn genuine AUC so far so saving model\")\n",
    "                self.bestAucCv = aucCv\n",
    "\n",
    "                # **TODO: Add your own logging/model saving/record keeping code here.\n",
    "                self.model.save(\"best_auc_model.hdf5\", overwrite=True)\n",
    "\n",
    "            vl = logs.get('val_loss')\n",
    "            if (self.bestCvLoss < vl) :\n",
    "                pass\n",
    "                #print(\"Best val loss on SoftAUC so far\")\n",
    "                #**TODO -  Add your own logging/saving/record keeping code here.\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # logs include loss, and optionally acc( if accuracy monitoring is enabled).\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import theano\n",
    "\n",
    "def pair_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    parts = tf.dynamic_partition(y_pred, y_true, 2)\n",
    "    y_pos = parts[1]\n",
    "    y_neg = parts[0]\n",
    "    y_pos = tf.expand_dims(y_pos, 0)\n",
    "    y_neg = tf.expand_dims(y_neg, -1)\n",
    "    out = K.sigmoid(y_neg - y_pos)\n",
    "    return K.mean(out)\n",
    "\n",
    "def pair_loss(y_true, y_pred):\n",
    "    # Extract 1s\n",
    "    pos_pred_vr = y_pred[y_true.nonzero()]\n",
    "    # Extract zeroes\n",
    "    neg_pred_vr = y_pred[theano.tensor.eq(y_true, 0).nonzero()]\n",
    "    # Broadcast the subtraction to give a matrix of differences  between pairs of observations.\n",
    "    pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)\n",
    "    # Get signmoid of each pair.\n",
    "    stats = theano.tensor.nnet.sigmoid(pred_diffs_vr * 2)\n",
    "    # Take average and reverse sign\n",
    "    return 1-theano.tensor.mean(stats) # as we want to minimise, and get this to zero\n",
    "\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)    \n",
    "    return FP/N\n",
    "\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)    \n",
    "    return TP/P\n",
    "\n",
    "def auc_metric(y_true, y_pred):   \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return -K.sum(s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution1D, Activation, Merge, Reshape\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def batch_generator(X, y, BATCH_SIZE, EPOCH_PARTION):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet training\n",
    "    input:\n",
    "        X - train dataset, numpy array or csr matrix\n",
    "        y - target, numpy array\n",
    "        BATCH_SIZE - int, number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory.\n",
    "        EPOCH_PARTION - float. If in interval (0, 1) - share of objects that will be used for training in epoch.\n",
    "            Objects are chosen randomly. If equals to 1 - nnet will be trained on all samples without randomization.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_number = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    batches_per_epoch = np.ceil(X.shape[0]/BATCH_SIZE*EPOCH_PARTION)\n",
    "    \n",
    "    while True:\n",
    "        if EPOCH_PARTION==1:\n",
    "            batch_indexes = sample_index[BATCH_SIZE*batch_number : BATCH_SIZE*(batch_number+1)]    \n",
    "        else:\n",
    "            batch_indexes = np.random.choice(X.shape[0], BATCH_SIZE)\n",
    "        \n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        y_batch = to_categorical(y, num_classes=2)[batch_indexes]\n",
    "        \n",
    "        batch_number += 1\n",
    "        if batch_number == batches_per_epoch-1:\n",
    "            batch_number = 0\n",
    "        yield X_batch, y_batch\n",
    "            \n",
    "def batch_generator_p(X, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet predicitons\n",
    "    input:\n",
    "        X - train dataset,  numpy array or csr matrix\n",
    "        BATCH_SIZE - number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory        \n",
    "    \"\"\"\n",
    "    batches_per_epoch = np.ceil(X.shape[0]/BATCH_SIZE)\n",
    "    batch_number = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_indexes = sample_index[BATCH_SIZE*batch_number : BATCH_SIZE*(batch_number+1)]\n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        batch_number += 1\n",
    "        yield (X_batch)\n",
    "        if batch_number == batches_per_epoch:\n",
    "            batch_number = 0\n",
    "            \n",
    "def compile_nnet(data, n0, d0, n1, n2, n3, d1, d2, d3, regul, use_batch_norm, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to compile simple nnet. Architecture is self-explanatory with code\n",
    "    input:\n",
    "        data - numpy arary or csr matrix for training\n",
    "        n1, n2 - ints, number of neurons in first and second layers\n",
    "        d1, d2 - float, dropouts in first and second layers\n",
    "        regul - float, regularization paramter, the same for both layers\n",
    "        parameters might be passed as a dictionary\n",
    "    output:\n",
    "        nnet model\n",
    "    \"\"\"\n",
    "    models = []\n",
    "\n",
    "    model_ps_car_01_cat = Sequential()\n",
    "    model_ps_car_01_cat.add(Embedding(13, 7, input_length=1))\n",
    "    model_ps_car_01_cat.add(Reshape(target_shape=(7,)))\n",
    "    models.append(model_ps_car_01_cat)\n",
    "    \n",
    "    model_ps_car_02_cat = Sequential()\n",
    "    model_ps_car_02_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_02_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_02_cat)\n",
    "    \n",
    "    model_ps_car_03_cat = Sequential()\n",
    "    model_ps_car_03_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_03_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_03_cat)\n",
    "    \n",
    "    model_ps_car_04_cat = Sequential()\n",
    "    model_ps_car_04_cat.add(Embedding(10, 5, input_length=1))\n",
    "    model_ps_car_04_cat.add(Reshape(target_shape=(5,)))\n",
    "    models.append(model_ps_car_04_cat)\n",
    "    \n",
    "    model_ps_car_05_cat = Sequential()\n",
    "    model_ps_car_05_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_05_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_05_cat)\n",
    "    \n",
    "    model_ps_car_06_cat = Sequential()\n",
    "    model_ps_car_06_cat.add(Embedding(18, 6, input_length=1))\n",
    "    model_ps_car_06_cat.add(Reshape(target_shape=(6,)))\n",
    "    models.append(model_ps_car_06_cat)\n",
    "    \n",
    "    model_ps_car_07_cat = Sequential()\n",
    "    model_ps_car_07_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_07_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_07_cat)\n",
    "    \n",
    "    model_ps_car_09_cat = Sequential()\n",
    "    model_ps_car_09_cat.add(Embedding(6, 3, input_length=1))\n",
    "    model_ps_car_09_cat.add(Reshape(target_shape=(3,)))\n",
    "    models.append(model_ps_car_09_cat)\n",
    "    \n",
    "    model_ps_car_10_cat = Sequential()\n",
    "    model_ps_car_10_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_10_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_10_cat)\n",
    "    \n",
    "    model_ps_car_11_cat = Sequential()\n",
    "    model_ps_car_11_cat.add(Embedding(104, 10, input_length=1))\n",
    "    model_ps_car_11_cat.add(Reshape(target_shape=(10,)))\n",
    "    models.append(model_ps_car_11_cat)\n",
    "    \n",
    "    \n",
    "    model_ps_ind_02_cat = Sequential()\n",
    "    model_ps_ind_02_cat.add(Embedding(5, 3, input_length=1))\n",
    "    model_ps_ind_02_cat.add(Reshape(target_shape=(3,)))\n",
    "    models.append(model_ps_ind_02_cat)\n",
    "    \n",
    "    model_ps_ind_04_cat = Sequential()\n",
    "    model_ps_ind_04_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_ind_04_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_ind_04_cat)\n",
    "    \n",
    "    model_ps_ind_05_cat = Sequential()\n",
    "    model_ps_ind_05_cat.add(Embedding(8, 5, input_length=1))\n",
    "    model_ps_ind_05_cat.add(Reshape(target_shape=(5,)))\n",
    "    models.append(model_ps_ind_05_cat)  \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    model_rest = Sequential()\n",
    "    model_rest.add(Dense(n0, input_dim=24))\n",
    "    model_rest.add(Dropout(d0))\n",
    "    models.append(model_rest)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Merge(models, mode='concat'))\n",
    "    \n",
    "    \n",
    "    if regul>0:\n",
    "        model.add(Dense(n1, \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n1)),#data.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(d1))\n",
    "    \n",
    "    if regul>0:\n",
    "        model.add(Dense(n2, \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n2))\n",
    "    model.add(PReLU())\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(d2))\n",
    "    \n",
    "    if regul>0 and n3>0:\n",
    "        model.add(Dense(n3, \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "        model.add(PReLU())\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization(axis=1))\n",
    "        model.add(Dropout(d3))\n",
    "    elif n3>0:    \n",
    "        model.add(Dense(n3))\n",
    "        model.add(PReLU())\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization(axis=1))\n",
    "        model.add(Dropout(d3))\n",
    "    \n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    #adam = Adam()\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
    "    \n",
    "    #model.compile(loss=pair_loss, metrics=[pair_loss], optimizer='adam') \n",
    "    model.compile(loss='binary_crossentropy', metrics=[auc_metric], optimizer='adam') \n",
    "    \n",
    "    return model\n",
    "\n",
    "# def nnet_pred(params, train, ytrain, valid, yvalid, \n",
    "#              # test_fold, \n",
    "#               kagg):\n",
    "#     \"\"\"\n",
    "#     input:\n",
    "#         params - dictionary of parameters to be passed to function compile_nnet. May also contain size of a \n",
    "#             batch and share of objects per epoch\n",
    "#         train, valid, test_fold, kagg - numpy arrays or csr matrices. Nnet is trained on train data, best \n",
    "#             number of epochs is chosen by binary_crossentropy loss on valid. Best model is saved every epoch \n",
    "#             and is loaded if there was no improvement on valid set for 10 epochs in a row. Test_fold and \n",
    "#             kagg - matrices, for which predictions are returned.\n",
    "#         ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "#     output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if params.get('use_scaler', False):\n",
    "#         if type(train).__name__ == 'csr_matrix':\n",
    "#             scaler = MaxAbsScaler()\n",
    "#         else:\n",
    "#             scaler = MinMaxScaler()\n",
    "#         train = scaler.fit_transform(train)\n",
    "#         valid = scaler.transform(valid)\n",
    "#         test_fold  = scaler.transform(test_fold)\n",
    "#         kagg  = scaler.transform(kagg)\n",
    "    \n",
    "#     model = compile_nnet(train, **params)\n",
    "#     early_stopper = EarlyStopping(monitor='val_binary_crossentropy', patience=10, verbose=0, mode='auto')\n",
    "#     checkpoint = ModelCheckpoint(filepath='nnet_checkpoint.hdf5', \n",
    "#                                  monitor='val_binary_crossentropy', \n",
    "#                                  save_best_only=True)\n",
    "\n",
    "#     BATCH_SIZE = params.get('BATCH_SIZE', 256)\n",
    "#     EPOCH_PARTION = params.get('EPOCH_PARTION', 1)\n",
    "#     #nb_epoch\n",
    "#     model.fit(train, ytrain, sample_weight=np.array([1 if i==0 else 5 for i in ytrain]),\n",
    "#               epochs=500, batch_size=BATCH_SIZE, verbose=0,\n",
    "\n",
    "#                         validation_data=(valid, yvalid, np.array([1 if i==0 else 5 for i in yvalid])),\n",
    "#                         callbacks=[early_stopper, checkpoint]\n",
    "#              )\n",
    "    \n",
    "#     model.load_weights('nnet_checkpoint.hdf5') \n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
    "    \n",
    "#     fold_pred = model.predict(valid)\n",
    "#     kagg_pred = model.predict(kagg)\n",
    "    \n",
    "#     return fold_pred.reshape(-1), kagg_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def nnet_pred(params, train, ytrain, valid, yvalid, \n",
    "             # test_fold, \n",
    "              kagg):\n",
    "    \n",
    "    model = compile_nnet(train, **params)\n",
    "    early_stopper = EarlyStopping(monitor='val_auc_metric', patience=4, verbose=0, mode='min')\n",
    "    checkpoint = ModelCheckpoint(filepath='nnet_checkpoint.hdf5', \n",
    "                                 monitor='val_auc_metric', \n",
    "                                 save_best_only=True)\n",
    "\n",
    "    BATCH_SIZE = params.get('BATCH_SIZE', 256)\n",
    "    EPOCH_PARTION = params.get('EPOCH_PARTION', 1)\n",
    "    #nb_epoch\n",
    "    model.fit(train, ytrain, sample_weight=np.array([1 if i==0 else 5 for i in ytrain]),\n",
    "              epochs=500, batch_size=BATCH_SIZE, verbose=0,\n",
    "\n",
    "                        validation_data=(valid, yvalid),# np.array([1 if i==0 else 5 for i in yvalid])),\n",
    "                        callbacks=[early_stopper, checkpoint]\n",
    "             )\n",
    "    \n",
    "    model.load_weights('nnet_checkpoint.hdf5') \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc_metric])\n",
    "    \n",
    "    fold_pred = model.predict(valid)\n",
    "    kagg_pred = model.predict(kagg)\n",
    "    \n",
    "    return fold_pred.reshape(-1), kagg_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnet_pred(params, train, ytrain, valid, yvalid, \n",
    "             # test_fold, \n",
    "              kagg):\n",
    "    \n",
    "    model = compile_nnet(train, **params)\n",
    "    \n",
    "    callbacksList = [AUC_SKlearn_callback(train, ytrain, useCv=True)]\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_auc_metric', min_delta=0.00001, patience=6,\n",
    "                                                       verbose=2, mode='min')\n",
    "    callbacksList.append( early_stopping )\n",
    "    \n",
    "\n",
    "    BATCH_SIZE = params.get('BATCH_SIZE', 256)\n",
    "    EPOCH_PARTION = params.get('EPOCH_PARTION', 1)\n",
    "    SCALE_POS_WEIGHT = params.get('scale_pos_weight', 1)\n",
    "    model.fit(train, ytrain,  sample_weight=np.array([1 if i==0 else SCALE_POS_WEIGHT for i in ytrain]),\n",
    "              epochs=100, batch_size=BATCH_SIZE, verbose=0,\n",
    "\n",
    "             validation_data=(valid, yvalid),\n",
    "             callbacks=callbacksList\n",
    "             )\n",
    "    \n",
    "    model.load_weights('best_auc_model.hdf5') \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc_metric])\n",
    "    \n",
    "    fold_pred = model.predict(valid)\n",
    "    kagg_pred = model.predict(kagg)\n",
    "    \n",
    "    return fold_pred.reshape(-1), kagg_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def xgb_pred(params, train, ytrain, valid, yvalid, test_fold, kagg):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        params - dictionary of parameters to be passed to xgb.train\n",
    "        train, valid, test_fold, kagg - numpy arrays or csr matrices. Model is trained on train data, best \n",
    "            number of epochs is chosen by loss on valid. Test_fold and kagg - matrices, for which predictions \n",
    "            are returned.\n",
    "        ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "    output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    train  = xgb.DMatrix(train, ytrain)\n",
    "    dvalid = xgb.DMatrix(valid, yvalid)\n",
    "    watchlist = [(train, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "    boost = xgb.train(params, train, \n",
    "                    num_boost_round=10000, \n",
    "                    evals=watchlist,\n",
    "                    verbose_eval=False,\n",
    "                    maximize=True,\n",
    "                    early_stopping_rounds=50)\n",
    "    \n",
    "    # if we trained a linear model, then it has no ntree_limit parameter\n",
    "    if params['booster'] == 'gbtree':\n",
    "        fold_pred = boost.predict(xgb.DMatrix(test_fold), ntree_limit=boost.best_iteration)\n",
    "        kagg_pred = boost.predict(xgb.DMatrix(kagg),      ntree_limit=boost.best_iteration)\n",
    "        print (boost.best_iteration)\n",
    "    else:\n",
    "        fold_pred = boost.predict(xgb.DMatrix(test_fold))\n",
    "        kagg_pred = boost.predict(xgb.DMatrix(kagg))\n",
    "        \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def lgb_pred(params, train, ytrain, valid, yvalid, test_fold, kagg):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        params - dictionary of parameters to be passed to lgb.train\n",
    "        train, valid, test_fold, kagg - numpy arrays or csr matrices. Model is trained on train data, best \n",
    "            number of epochs is chosen by loss on valid. Test_fold and kagg - matrices, for which predictions \n",
    "            are returned.\n",
    "        ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "    output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    dtrain = lgb.Dataset(train, ytrain)\n",
    "    dvalid = lgb.Dataset(valid, yvalid, reference=dtrain)\n",
    "    \n",
    "    gbm = lgb.train(params, dtrain,\n",
    "                    num_boost_round=100000,\n",
    "                    valid_sets=[dtrain, dvalid],\n",
    "                    verbose_eval=False,\n",
    "                    early_stopping_rounds=50)\n",
    "    \n",
    "    fold_pred = gbm.predict(test_fold, num_iteration=gbm.best_iteration)\n",
    "    kagg_pred = gbm.predict(kagg,      num_iteration=gbm.best_iteration) \n",
    "    print (gbm.best_iteration)\n",
    "    \n",
    "    #train_pred = gbm.predict(train, num_iteration=gbm.best_iteration)\n",
    "    #valid_pred = gbm.predict(valid, num_iteration=gbm.best_iteration)\n",
    "    #print (gini(ytrain, train_pred)/gini(ytrain, ytrain))\n",
    "    #print (gini(yvalid, valid_pred)/gini(yvalid, yvalid))\n",
    "    \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def fastfm_pred(params, train, ytrain, test_fold, kagg):\n",
    "    fmc = FMClassification(**params)\n",
    "    fmc.fit(train, ytrain)\n",
    "    fold_pred = fmc.predict_proba(test)\n",
    "    kagg_pred = fmc.predict_proba(kagg)\n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def mod_stat(y, train_duperate):\n",
    "    test_duperate = 0.5\n",
    "    a = test_duperate/train_duperate\n",
    "    b = (1-test_duperate) / (1-train_duperate)\n",
    "    return a*y / (a*y + b*(1-y))\n",
    "\n",
    "    \n",
    "def get_oofs(model, data, kagg, y, features=[], cat_features=[], n_splits=5, iters_total=2, \n",
    "             params=None, valid_size=0, SEED=100, ykagg=None):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model: string ('lgb'/'fastfm'/'xgb'/'nnet') or any sklearn model\n",
    "        data: train data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        kagg: test data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        y: target for train data in a form of pandas DataFrame / pandas Series / numpay array\n",
    "        ids: graph ids in a form of pandas Series. Graphs ids are used to split train data into separate graphs \n",
    "            to prevent overfitting. In other words, quesitons of the same graph will always be in one fold.\n",
    "        n_splits: number of splits for train data. Default value is 5. In order to get one OOF prediciton, \n",
    "            model must be fitted n_splits times\n",
    "        iters_total: number of total iterations. Default value is 3. OOFs then will be blended. \n",
    "            Total number of times a model will be fitted is n_splits*iters_total.\n",
    "        params: model parameters in a form of dictionary.\n",
    "    output: \n",
    "        data_oofs: numpy array of OOF predictions for train data. Result is blended iters_total times\n",
    "        kagg_oofs: numpy array of predictions for train data. Result is blended n_splits*iters_total times\n",
    "    \"\"\"\n",
    "        \n",
    "    if type(data).__name__=='DataFrame':\n",
    "        if len(features) == 0:\n",
    "            features = data.columns\n",
    "        data = data[features]#.values\n",
    "    if type(kagg).__name__=='DataFrame':\n",
    "        if len(features) == 0:\n",
    "            features = kagg.columns\n",
    "        kagg = kagg[features]#.values\n",
    "        \n",
    "    if model == 'fastfm':\n",
    "        y = y.replace(0, -1)\n",
    "        if type(data).__name__ != 'csr_matrix':\n",
    "            data = sparse.csr_matrix(data)\n",
    "            kagg = sparse.csr_matrix(kagg)\n",
    "    if type(y).__name__=='Series' or type(y).__name__=='DataFrame':\n",
    "        y = y.values\n",
    "    \n",
    "    # matrices to store preditions\n",
    "    data_oofs = np.zeros((data.shape[0]))\n",
    "    kagg_oofs = np.zeros((kagg.shape[0]))\n",
    "    \n",
    "    data_oofs_ranks = np.zeros((data.shape[0]))\n",
    "    kagg_oofs_ranks = np.zeros((kagg.shape[0]))\n",
    "    \n",
    "    for iter_num in range(iters_total):\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=iter_num+SEED)\n",
    "        for train_ind, test_ind in kf.split(data, y):\n",
    "            kagg_fold = kagg\n",
    "            \n",
    "            # Adding validation sets (from train set) for models that require it. \n",
    "            # Validation size is 12.5% of train fold.\n",
    "            if (model=='xgb' or model=='lgb' or model=='nnet') and valid_size>0:\n",
    "                kf_valid = StratifiedKFold(n_splits=int(1/valid_size), shuffle=True, random_state=iter_num+SEED)\n",
    "                train_data, ytrain = data[train_ind], y[train_ind]\n",
    "                train_ind, valid_ind = list(kf_valid.split(train_data, ytrain))[0]\n",
    "                valid_data, yvalid = train_data[valid_ind], ytrain.iloc[valid_ind]\n",
    "                train_data, ytrain = train_data[train_ind], ytrain.iloc[train_ind]\n",
    "                \n",
    "            else:\n",
    "                train_data, ytrain = data.iloc[train_ind], y[train_ind]\n",
    "                valid_data, yvalid = data.iloc[test_ind],  y[test_ind]\n",
    "\n",
    "            train_data = train_data.reset_index(drop=True)\n",
    "            valid_data = valid_data.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            pos = (pd.Series(ytrain == 1))\n",
    "            train_data = pd.concat([train_data, train_data.loc[pos], \n",
    "                                    train_data.loc[pos], train_data.loc[pos]], axis=0)\n",
    "            ytrain = np.concatenate([ytrain, ytrain[pos], ytrain[pos], ytrain[pos]])\n",
    "            idx = np.arange(len(train_data))\n",
    "            np.random.shuffle(idx)\n",
    "            train_data = train_data.iloc[idx]\n",
    "            ytrain = ytrain[idx]\n",
    "\n",
    "\n",
    "            proc_train_data, proc_valid_data, proc_kagg_fold = preproc(train_data, valid_data, kagg_fold)\n",
    "            new_features = list(set(features) - set(cat_features)) + list(map(lambda x: x+'_enc', cat_features))\n",
    "\n",
    "\n",
    "            if model=='lgb':\n",
    "                fold_pred, kagg_pred = lgb_pred (params, train_data[new_features], ytrain, \n",
    "                                                         valid_data[new_features], yvalid,\n",
    "                                                         data.iloc[test_ind][new_features], \n",
    "                                                         kagg_fold[new_features])\n",
    "            elif model=='xgb':\n",
    "                fold_pred, kagg_pred = xgb_pred (params, train_data[new_features], ytrain, \n",
    "                                                         valid_data[new_features], yvalid,\n",
    "                                                         data.iloc[test_ind][new_features], \n",
    "                                                         kagg_fold[new_features])\n",
    "            elif model=='nnet':\n",
    "                fold_pred, kagg_pred = nnet_pred(params, proc_train_data, ytrain, \n",
    "                                                         proc_valid_data, yvalid,\n",
    "                                                         #data.iloc [test_ind][new_features], \n",
    "                                                         proc_kagg_fold)\n",
    "            elif model=='fastfm':\n",
    "                fold_pred, kagg_pred = fastfm_pred(params, data[train_ind], y[train_ind], data[test_ind], kagg)\n",
    "\n",
    "            # Block for working with sklearn models\n",
    "            else:\n",
    "                model.fit(data[train_ind], y[train_ind])\n",
    "                try:\n",
    "                    fold_pred = model.predict_proba(data[test_ind])[:,1]\n",
    "                    kagg_pred = model.predict_proba(kagg)[:,1]\n",
    "                except:\n",
    "                    try:\n",
    "                        fold_pred = model.predict_proba(data[test_ind])\n",
    "                        kagg_pred = model.predict_proba(kagg)\n",
    "                    except:\n",
    "                        fold_pred = model.predict(data[test_ind])\n",
    "                        kagg_pred = model.predict(kagg)\n",
    "                        \n",
    "            data_oofs[test_ind] += mod_stat(fold_pred, np.mean(fold_pred))\n",
    "            kagg_oofs += mod_stat(kagg_pred, np.mean(kagg_pred))\n",
    "            \n",
    "            data_oofs_ranks[test_ind] += fold_pred.argsort().argsort()\n",
    "            kagg_oofs_ranks += kagg_pred.argsort().argsort()\n",
    "            \n",
    "\n",
    "            try:\n",
    "                print ('fold loss: ', round(gini(ykagg, kagg_pred)/gini(ykagg, ykagg), 4),\n",
    "                   round(np.mean(kagg_pred), 4))\n",
    "            except:\n",
    "                print ('fold loss: ', round(gini(y[test_ind], fold_pred)/gini(y[test_ind], y[test_ind]), 4),\n",
    "                    round(np.mean(kagg_pred), 4))\n",
    "    \n",
    "        print ('iteration OOF score:', gini(y, data_oofs/(iter_num+1))/gini(y, y))\n",
    "    data_oofs /= iters_total\n",
    "    kagg_oofs /= (iters_total*n_splits)\n",
    "    return data_oofs, kagg_oofs, data_oofs_ranks, kagg_oofs_ranks\n",
    "\n",
    "train_preds = pd.DataFrame()\n",
    "test_preds  = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "col = 'nnet'\n",
    "params = {'n0': 32, 'd0':0.25, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 5, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=1\n",
    "                                ,params=params\n",
    "                                ,ykagg=target_test#[:10000]\n",
    "                                ,SEED=2\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "col = 'nnet'\n",
    "params = {'n0': 32, 'd0':0.25, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 10, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=1\n",
    "                                ,params=params\n",
    "                                ,ykagg=target_test#[:10000]\n",
    "                                ,SEED=2\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "col = 'nnet'\n",
    "params = {'n0': 32, 'd0':0.25, \n",
    "          'n1':64, 'n2':32, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 5, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=1\n",
    "                                ,params=params\n",
    "                                ,ykagg=target_test#[:10000]\n",
    "                                ,SEED=2\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# loss - logloss, stop by auc\n",
    "params = {'n0': 32, 'd0':0.0, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': True, 'scale_pos_weight': 5, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "test_10, cv 2763 \n",
    "\n",
    "\n",
    "# early stopping and loss - logloss\n",
    "params = {'n0': 32, 'd0':0.25, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "test 11, cv 2756\n",
    "\n",
    "\n",
    "# early stopping and loss - logloss\n",
    "params = {'n0': 16, 'd0':0.2, \n",
    "          'n1': 64, 'n2':32, 'n3':16,\n",
    "          'd1':0.3, 'd2':0.2, 'd3':0.2, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "test 15, cv 2701\n",
    "\n",
    "#early stopping - auc\n",
    "params = {'n0': 16, 'd0':0.2, \n",
    "          'n1': 64, 'n2':32, 'n3':16,\n",
    "          'd1':0.3, 'd2':0.2, 'd3':0.2, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "test 16, cv 2733\n",
    "\n",
    "\n",
    "#early stopping - auc, discr\n",
    "params = {'n0': 32, 'd0':0.0, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 5, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "test 17, cv 2811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# early stopping and los - logloss\n",
    "col = 'target'\n",
    "params = {'n0': 32, 'd0':0.25, \n",
    "          'n1':128, 'n2':64, 'n3':0,\n",
    "          'd1':0.25, 'd2':0.25, 'd3':0, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=2\n",
    "                                ,params=params\n",
    "                                ,ykagg=None#target_test#[:10000]\n",
    "                                ,SEED=100\n",
    "                                 )\n",
    "test_preds[[ 'target']].to_csv('test_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# early stopping and los - logloss\n",
    "col = 'target'\n",
    "params = {'n0': 16, 'd0':0.2, \n",
    "          'n1': 64, 'n2':32, 'n3':16,\n",
    "          'd1':0.3, 'd2':0.2, 'd3':0.2, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=2\n",
    "                                ,params=params\n",
    "                                ,ykagg=None#target_test#[:10000]\n",
    "                                ,SEED=200\n",
    "                                 )\n",
    "test_preds[[ 'target']].to_csv('test_15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#early stopping - auc\n",
    "col = 'target'\n",
    "params = {'n0': 16, 'd0':0.2, \n",
    "          'n1': 64, 'n2':32, 'n3':16,\n",
    "          'd1':0.3, 'd2':0.2, 'd3':0.2, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 20, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=2\n",
    "                                ,params=params\n",
    "                                ,ykagg=None#target_test#[:10000]\n",
    "                                ,SEED=200\n",
    "                                 )\n",
    "test_preds[['target']].to_csv('test_16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#early stopping - auc, oversampling 1+3\n",
    "col = 'target'\n",
    "params = {'n0': 16, 'd0':0.2, \n",
    "          'n1': 64, 'n2':32, 'n3':16,\n",
    "          'd1':0.3, 'd2':0.2, 'd3':0.2, 'regul':0, \n",
    "          'use_batch_norm': False, 'scale_pos_weight': 5, \n",
    "          'BATCH_SIZE':4096, 'EPOCH_PARTION':1, 'use_scaler':False}\n",
    "\n",
    "train_preds[col], test_preds[col], train_preds[col+'_rank'], \\\n",
    "test_preds[col+'_rank']= get_oofs('nnet'\n",
    "                                ,train#.iloc[:10000]\n",
    "                                ,test#.iloc[:10000]\n",
    "                                ,target_train#[:10000]\n",
    "                                ,list(train.columns)#sorted(list(set(features)-set([''])))\n",
    "                                ,cat_features=[]\n",
    "                                ,n_splits=5\n",
    "                                ,iters_total=2\n",
    "                                ,params=params\n",
    "                                ,ykagg=None#target_test#[:10000]\n",
    "                                ,SEED=200\n",
    "                                 )\n",
    "test_preds[['target']].to_csv('test_17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
